{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'the': 1,\n 'cat': 2,\n 'sat': 3,\n 'on': 4,\n 'mat': 5,\n 'dog': 6,\n 'ate': 7,\n 'my': 8,\n 'homework': 9}"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "samples = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ate my homework\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "max_features = 10000\n",
    "max_length = 20\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD EMBEDDINGS\n",
    "one hot vectors are wasteful of space, and can not scale to very large differences in values. \n",
    "word embeddings are associating every word with a float vector. \n",
    "This allows us to represent a very large variety of words with much less space (quasi logarithmic)\n",
    "A good word embedding should show structure shown in the words and their relationships.\n",
    "\n",
    "Example: \n",
    "If (dog, wolf, book, notes) are in the dataset, their vectors in n-D space should be representing similarity.\n",
    "dog and wolf should have less euclidean distance than suppose say dog and book. \n",
    "similarly book and notes should have less distance. \n",
    "\n",
    "\n",
    "Initially, the Embeddings are assigned randomly, and with training process, we use backpropagation to get better embeddings. \n",
    "If we are working on a document task, and we can get a good word embeddings trained on millions of sentences, we should use that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 20, 8)             80000     \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 160)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 161       \n=================================================================\nTotal params: 80,161\nTrainable params: 80,161\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n625/625 [==============================] - 2s 3ms/step - loss: 0.6704 - acc: 0.6173 - val_loss: 0.6210 - val_acc: 0.6948\nEpoch 2/10\n625/625 [==============================] - 2s 3ms/step - loss: 0.5420 - acc: 0.7523 - val_loss: 0.5254 - val_acc: 0.7322\nEpoch 3/10\n625/625 [==============================] - 2s 3ms/step - loss: 0.4616 - acc: 0.7837 - val_loss: 0.5010 - val_acc: 0.7480\nEpoch 4/10\n625/625 [==============================] - 2s 2ms/step - loss: 0.4240 - acc: 0.8066 - val_loss: 0.4946 - val_acc: 0.7558\nEpoch 5/10\n625/625 [==============================] - 2s 3ms/step - loss: 0.3984 - acc: 0.8202 - val_loss: 0.4955 - val_acc: 0.7592\nEpoch 6/10\n625/625 [==============================] - 2s 3ms/step - loss: 0.3776 - acc: 0.8310 - val_loss: 0.4994 - val_acc: 0.7572\nEpoch 7/10\n625/625 [==============================] - 2s 2ms/step - loss: 0.3594 - acc: 0.8422 - val_loss: 0.5021 - val_acc: 0.7590\nEpoch 8/10\n625/625 [==============================] - 1s 2ms/step - loss: 0.3415 - acc: 0.8523 - val_loss: 0.5070 - val_acc: 0.7592\nEpoch 9/10\n625/625 [==============================] - 2s 2ms/step - loss: 0.3241 - acc: 0.8616 - val_loss: 0.5128 - val_acc: 0.7572\nEpoch 10/10\n625/625 [==============================] - 1s 2ms/step - loss: 0.3075 - acc: 0.8721 - val_loss: 0.5200 - val_acc: 0.7522\n"
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can easily be increased upto around 90% accuracy by just increasing max_length = 200 words instead. \n",
    "But since this network has only one Dense neuron, \n",
    "it is essentially weighted sum of everything before. \n",
    "sum(x(i) * w(i)) does not take into account relationships between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "\n",
    "raw = \"aclImdb\"\n",
    "base_dir = pjoin(os.getcwd(), raw)\n",
    "train_dir = pjoin(base_dir, 'train')\n",
    "test_dir = pjoin(base_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dirname = pjoin(train_dir, label_type)\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename[-4:] == '.txt':\n",
    "            contents = open(pjoin(dirname, filename),encoding='utf-8').read()\n",
    "            texts.append(contents)\n",
    "            labels.append(1 if label_type == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 88582 words\n"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# assign number to each word\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# convert every sentence into an array of numbers\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(f\"Found {len(tokenizer.word_index)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((25000, 100), (25000,))"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train on only 200 samples, and use a pretrained word embedding to get good performance\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "X_train = data[:training_samples]\n",
    "Y_train = labels[:training_samples]\n",
    "\n",
    "X_val = data[training_samples: training_samples + validation_samples]\n",
    "Y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors\n"
    }
   ],
   "source": [
    "# Download the pretrained word embedding -> https://nlp.stanford.edu/projects/glove/\n",
    "glove_dir = pjoin(os.getcwd(), 'glove.6B')\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(pjoin(glove_dir, \"glove.6B.100d.txt\"), encoding='utf-8') as reader:\n",
    "    for line in reader:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefficients = values[1:]\n",
    "        embeddings_index[word] = coefficients\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 100, 100)          1000000   \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 10000)             0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 32)                320032    \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 1,320,065\nTrainable params: 1,320,065\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n7/7 [==============================] - 1s 95ms/step - loss: 2.0398 - acc: 0.4800 - val_loss: 0.7291 - val_acc: 0.4967\nEpoch 2/10\n7/7 [==============================] - 1s 89ms/step - loss: 0.5055 - acc: 0.7950 - val_loss: 1.2128 - val_acc: 0.4995\nEpoch 3/10\n7/7 [==============================] - 1s 89ms/step - loss: 0.5465 - acc: 0.7150 - val_loss: 0.7197 - val_acc: 0.5558\nEpoch 4/10\n7/7 [==============================] - 1s 84ms/step - loss: 0.3306 - acc: 0.8650 - val_loss: 0.7516 - val_acc: 0.5566\nEpoch 5/10\n7/7 [==============================] - 1s 89ms/step - loss: 0.2502 - acc: 0.9150 - val_loss: 0.7906 - val_acc: 0.5509\nEpoch 6/10\n7/7 [==============================] - 1s 95ms/step - loss: 0.1225 - acc: 1.0000 - val_loss: 1.0304 - val_acc: 0.5213\nEpoch 7/10\n7/7 [==============================] - 1s 91ms/step - loss: 0.1437 - acc: 0.9750 - val_loss: 1.3921 - val_acc: 0.5008\nEpoch 8/10\n7/7 [==============================] - 1s 85ms/step - loss: 0.1404 - acc: 0.9550 - val_loss: 0.9632 - val_acc: 0.5480\nEpoch 9/10\n7/7 [==============================] - 1s 83ms/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.7958 - val_acc: 0.5693\nEpoch 10/10\n7/7 [==============================] - 1s 83ms/step - loss: 0.0232 - acc: 1.0000 - val_loss: 0.8526 - val_acc: 0.5681\n"
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train, \n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_val, Y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}